{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-05T13:55:48.533418Z",
     "start_time": "2025-03-05T13:55:47.227540Z"
    }
   },
   "source": [
    "import os\n",
    "from esm.models.esmc import ESMC\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig\n",
    "import torch\n",
    "import math\n",
    "import tqdm"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "a190fdb085744e25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T13:55:49.583071Z",
     "start_time": "2025-03-05T13:55:48.540981Z"
    }
   },
   "source": [
    "import_data = \"../Data/Protein_Gym_Datasets/GFP_AEQVI_Sarkisyan_2016.csv\"\n",
    "data_set = import_data.split(\"/\").split(\".\")[0]\n",
    "#HIS7_YEAST_Pokusaeva_2019\n",
    "input_data = []\n",
    "with open(import_data, \"r\") as infile:\n",
    "    for line in infile.readlines():\n",
    "        input_data.append(line[:-1].split(\",\"))\n",
    "\n",
    "to_encode = [line[1] for line in input_data[1:]]"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "7a5a3e14619e2b3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T13:58:24.044966Z",
     "start_time": "2025-03-05T13:58:22.280779Z"
    }
   },
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for method in [\"esmc_300m\",\"esmc_600m\"]:\n",
    "    pad = 1 + math.floor(math.log(len(to_encode), 10))\n",
    "    outfile = \"../Data/ESM_Embeddings\"\n",
    "    with tqdm.tqdm(total=len(to_encode)) as pbar:\n",
    "        for i, sequence in enumerate(to_encode):\n",
    "            protein = ESMProtein(sequence=sequence)\n",
    "            client = ESMC.from_pretrained(method).to(\"cuda\")  # or \"cpu\"\n",
    "            protein_tensor = client.encode(protein)\n",
    "            logits_output = client.logits(protein_tensor,\n",
    "                                        LogitsConfig(sequence=True, return_embeddings=True))\n",
    "\n",
    "            residue_embeddings = logits_output.embeddings[0, 1:-1, :]\n",
    "            protein_embedding = residue_embeddings.mean(dim=0)\n",
    "            outfile = f\"{i:0{pad}d}\"\n",
    "            torch.save(protein_embedding, f\"../Data/Embeddings/{data_set}/{method}/{outfile}\", _use_new_zipfile_serialization=False)\n",
    "            pbar.update(1)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/496137 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.74 GiB of which 26.56 MiB is free. Including non-PyTorch memory, this process has 7.36 GiB memory in use. Of the allocated memory 6.65 GiB is allocated by PyTorch, and 579.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 11\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, sequence \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(to_encode):\n\u001B[1;32m     10\u001B[0m     protein \u001B[38;5;241m=\u001B[39m ESMProtein(sequence\u001B[38;5;241m=\u001B[39msequence)\n\u001B[0;32m---> 11\u001B[0m     client \u001B[38;5;241m=\u001B[39m \u001B[43mESMC\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# or \"cpu\"\u001B[39;00m\n\u001B[1;32m     12\u001B[0m     protein_tensor \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mencode(protein)\n\u001B[1;32m     13\u001B[0m     logits_output \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mlogits(protein_tensor,\n\u001B[1;32m     14\u001B[0m                                 LogitsConfig(sequence\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, return_embeddings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n",
      "File \u001B[0;32m/opt/conda/conda/envs/MAP/lib/python3.12/site-packages/esm/models/esmc.py:86\u001B[0m, in \u001B[0;36mESMC.from_pretrained\u001B[0;34m(cls, model_name, device)\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m device \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     85\u001B[0m     device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 86\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mload_local_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m device\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m     88\u001B[0m     model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mbfloat16)\n",
      "File \u001B[0;32m/opt/conda/conda/envs/MAP/lib/python3.12/site-packages/esm/pretrained.py:135\u001B[0m, in \u001B[0;36mload_local_model\u001B[0;34m(model_name, device)\u001B[0m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m LOCAL_MODEL_REGISTRY:\n\u001B[1;32m    134\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not found in local model registry.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 135\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mLOCAL_MODEL_REGISTRY\u001B[49m\u001B[43m[\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/conda/envs/MAP/lib/python3.12/site-packages/esm/pretrained.py:74\u001B[0m, in \u001B[0;36mESMC_300M_202412\u001B[0;34m(device, use_flash_attn)\u001B[0m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mdevice(device):\n\u001B[1;32m     67\u001B[0m     model \u001B[38;5;241m=\u001B[39m ESMC(\n\u001B[1;32m     68\u001B[0m         d_model\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m960\u001B[39m,\n\u001B[1;32m     69\u001B[0m         n_heads\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m15\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     72\u001B[0m         use_flash_attn\u001B[38;5;241m=\u001B[39muse_flash_attn,\n\u001B[1;32m     73\u001B[0m     )\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m---> 74\u001B[0m state_dict \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     75\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_root\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mesmc-300\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata/weights/esmc_300m_2024_12_v0.pth\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     76\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     78\u001B[0m model\u001B[38;5;241m.\u001B[39mload_state_dict(state_dict)\n\u001B[1;32m     80\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[0;32m/opt/conda/conda/envs/MAP/lib/python3.12/site-packages/torch/serialization.py:1462\u001B[0m, in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[1;32m   1460\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m weights_only:\n\u001B[1;32m   1461\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1462\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_load\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1463\u001B[0m \u001B[43m            \u001B[49m\u001B[43mopened_zipfile\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1464\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1465\u001B[0m \u001B[43m            \u001B[49m\u001B[43m_weights_only_unpickler\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1466\u001B[0m \u001B[43m            \u001B[49m\u001B[43moverall_storage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moverall_storage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1467\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpickle_load_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1468\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1469\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m pickle\u001B[38;5;241m.\u001B[39mUnpicklingError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1470\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m pickle\u001B[38;5;241m.\u001B[39mUnpicklingError(_get_wo_message(\u001B[38;5;28mstr\u001B[39m(e))) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/conda/envs/MAP/lib/python3.12/site-packages/torch/serialization.py:1964\u001B[0m, in \u001B[0;36m_load\u001B[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001B[0m\n\u001B[1;32m   1962\u001B[0m \u001B[38;5;28;01mglobal\u001B[39;00m _serialization_tls\n\u001B[1;32m   1963\u001B[0m _serialization_tls\u001B[38;5;241m.\u001B[39mmap_location \u001B[38;5;241m=\u001B[39m map_location\n\u001B[0;32m-> 1964\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43munpickler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1965\u001B[0m _serialization_tls\u001B[38;5;241m.\u001B[39mmap_location \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1967\u001B[0m torch\u001B[38;5;241m.\u001B[39m_utils\u001B[38;5;241m.\u001B[39m_validate_loaded_sparse_tensors()\n",
      "File \u001B[0;32m/opt/conda/conda/envs/MAP/lib/python3.12/site-packages/torch/_weights_only_unpickler.py:512\u001B[0m, in \u001B[0;36mUnpickler.load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    504\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    505\u001B[0m         \u001B[38;5;28mtype\u001B[39m(pid) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m\n\u001B[1;32m    506\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(pid) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    507\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mserialization\u001B[38;5;241m.\u001B[39m_maybe_decode_ascii(pid[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstorage\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    508\u001B[0m     ):\n\u001B[1;32m    509\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m UnpicklingError(\n\u001B[1;32m    510\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOnly persistent_load of storage is allowed, but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpid[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    511\u001B[0m         )\n\u001B[0;32m--> 512\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpersistent_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpid\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    513\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m key[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m [BINGET[\u001B[38;5;241m0\u001B[39m], LONG_BINGET[\u001B[38;5;241m0\u001B[39m]]:\n\u001B[1;32m    514\u001B[0m     idx \u001B[38;5;241m=\u001B[39m (read(\u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m key[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m BINGET[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01melse\u001B[39;00m unpack(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<I\u001B[39m\u001B[38;5;124m\"\u001B[39m, read(\u001B[38;5;241m4\u001B[39m)))[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m/opt/conda/conda/envs/MAP/lib/python3.12/site-packages/torch/serialization.py:1928\u001B[0m, in \u001B[0;36m_load.<locals>.persistent_load\u001B[0;34m(saved_id)\u001B[0m\n\u001B[1;32m   1926\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1927\u001B[0m     nbytes \u001B[38;5;241m=\u001B[39m numel \u001B[38;5;241m*\u001B[39m torch\u001B[38;5;241m.\u001B[39m_utils\u001B[38;5;241m.\u001B[39m_element_size(dtype)\n\u001B[0;32m-> 1928\u001B[0m     typed_storage \u001B[38;5;241m=\u001B[39m \u001B[43mload_tensor\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1929\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_maybe_decode_ascii\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1930\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1932\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m typed_storage\n",
      "File \u001B[0;32m/opt/conda/conda/envs/MAP/lib/python3.12/site-packages/torch/serialization.py:1900\u001B[0m, in \u001B[0;36m_load.<locals>.load_tensor\u001B[0;34m(dtype, numel, key, location)\u001B[0m\n\u001B[1;32m   1895\u001B[0m         storage\u001B[38;5;241m.\u001B[39mbyteswap(dtype)\n\u001B[1;32m   1897\u001B[0m \u001B[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001B[39;00m\n\u001B[1;32m   1898\u001B[0m \u001B[38;5;66;03m# stop wrapping with TypedStorage\u001B[39;00m\n\u001B[1;32m   1899\u001B[0m typed_storage \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstorage\u001B[38;5;241m.\u001B[39mTypedStorage(\n\u001B[0;32m-> 1900\u001B[0m     wrap_storage\u001B[38;5;241m=\u001B[39m\u001B[43mrestore_location\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m   1901\u001B[0m     dtype\u001B[38;5;241m=\u001B[39mdtype,\n\u001B[1;32m   1902\u001B[0m     _internal\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m   1903\u001B[0m )\n\u001B[1;32m   1905\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m typed_storage\u001B[38;5;241m.\u001B[39m_data_ptr() \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1906\u001B[0m     loaded_storages[key] \u001B[38;5;241m=\u001B[39m typed_storage\n",
      "File \u001B[0;32m/opt/conda/conda/envs/MAP/lib/python3.12/site-packages/torch/serialization.py:1806\u001B[0m, in \u001B[0;36m_get_restore_location.<locals>.restore_location\u001B[0;34m(storage, location)\u001B[0m\n\u001B[1;32m   1805\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mrestore_location\u001B[39m(storage, location):\n\u001B[0;32m-> 1806\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdefault_restore_location\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/conda/envs/MAP/lib/python3.12/site-packages/torch/serialization.py:693\u001B[0m, in \u001B[0;36mdefault_restore_location\u001B[0;34m(storage, location)\u001B[0m\n\u001B[1;32m    673\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    674\u001B[0m \u001B[38;5;124;03mRestores `storage` using a deserializer function registered for the `location`.\u001B[39;00m\n\u001B[1;32m    675\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    690\u001B[0m \u001B[38;5;124;03m       all matching ones return `None`.\u001B[39;00m\n\u001B[1;32m    691\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    692\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _, _, fn \u001B[38;5;129;01min\u001B[39;00m _package_registry:\n\u001B[0;32m--> 693\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    694\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    695\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m/opt/conda/conda/envs/MAP/lib/python3.12/site-packages/torch/serialization.py:632\u001B[0m, in \u001B[0;36m_deserialize\u001B[0;34m(backend_name, obj, location)\u001B[0m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m location\u001B[38;5;241m.\u001B[39mstartswith(backend_name):\n\u001B[1;32m    631\u001B[0m     device \u001B[38;5;241m=\u001B[39m _validate_device(location, backend_name)\n\u001B[0;32m--> 632\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/conda/envs/MAP/lib/python3.12/site-packages/torch/storage.py:292\u001B[0m, in \u001B[0;36m_StorageBase.to\u001B[0;34m(self, device, non_blocking)\u001B[0m\n\u001B[1;32m    290\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(device, torch\u001B[38;5;241m.\u001B[39mdevice):\n\u001B[1;32m    291\u001B[0m     device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(device)\n\u001B[0;32m--> 292\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_to\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/conda/envs/MAP/lib/python3.12/site-packages/torch/_utils.py:99\u001B[0m, in \u001B[0;36m_to\u001B[0;34m(self, device, non_blocking)\u001B[0m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m (\n\u001B[1;32m     97\u001B[0m         \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_sparse\n\u001B[1;32m     98\u001B[0m     ), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msparse storage is not supported for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdevice\u001B[38;5;241m.\u001B[39mtype\u001B[38;5;241m.\u001B[39mupper()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m tensors\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 99\u001B[0m     untyped_storage \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mUntypedStorage\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    100\u001B[0m     untyped_storage\u001B[38;5;241m.\u001B[39mcopy_(\u001B[38;5;28mself\u001B[39m, non_blocking)\n\u001B[1;32m    101\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m untyped_storage\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.74 GiB of which 26.56 MiB is free. Including non-PyTorch memory, this process has 7.36 GiB memory in use. Of the allocated memory 6.65 GiB is allocated by PyTorch, and 579.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
