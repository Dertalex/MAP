{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:36:01.402645Z",
     "start_time": "2025-04-03T09:36:00.234246Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import src.generate_encodings as ge\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.generate_encodings'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m mean_squared_error\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DataLoader, TensorDataset\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgenerate_encodings\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mge\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mmath\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mrandom\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'src.generate_encodings'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T13:51:22.020986Z",
     "start_time": "2025-03-12T13:51:21.975422Z"
    }
   },
   "source": [
    "print(torch.cuda.is_available())\n",
    "torch.cuda.get_device_name(0)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4060 Ti'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T13:56:20.555632Z",
     "start_time": "2025-03-12T13:51:27.163555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Load ESM Representations\n",
    "Warning: Only use this block or the block below. Not both!\n",
    "\"\"\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "random.seed(0)\n",
    "\n",
    "data_set = \"HIS7_YEAST_Pokusaeva_2019\"\n",
    "method = \"esmc_600m\"\n",
    "representations = []\n",
    "repr_layer = \"-1\"\n",
    "scores = []\n",
    "mapping_file = f\"../Data/Embeddings/{data_set}/pairings.csv\"\n",
    "path_to_embeddings = f\"../Data/Embeddings//HIS7_YEAST_Pokusaeva_2019/{method}/{repr_layer}\"\n",
    "with open(mapping_file) as f:\n",
    "    data_pairs = [line[:-1] for line in f.readlines()[1:]]\n",
    "    random.shuffle(data_pairs)\n",
    "\n",
    "with tqdm(total=len(data_pairs)) as pbar:\n",
    "    for line in data_pairs:\n",
    "        line = line[:-1].split(',')\n",
    "        embedding = f\"{path_to_embeddings}/{line[0][1:]}\"\n",
    "        representations.append(torch.load(embedding).to(dtype=torch.float32).cpu())\n",
    "        scores.append(float(line[1]))\n",
    "        pbar.update(1)\n",
    "\n",
    "print(\"successfully loaded all embeddings and their scores to cpu\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 496137/496137 [04:53<00:00, 1691.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded all embeddings and their scores to cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Generate Sequence Encodings + OHE\n",
    "Warning: Only use this block or the block above. Not both!\n",
    "\"\"\"\n",
    "data_set = \"CAPSD_AAV2S_Sinai_2021\"\n",
    "data_file = f\"../Data/Protein_Gym_Datasets/{data_set}.csv\"\n",
    "\n",
    "representations = []\n",
    "scores = []\n",
    "embedding_type = \"georgiev\"\n",
    "with open(data_file) as f:\n",
    "    data_pairs = [(line[:-1].split(\",\")[1][1:], line[:-1].split(\",\")[2]) for line in f.readlines()[1:]]\n",
    "\n",
    "with tqdm(total=len(data_pairs)) as pbar:\n",
    "    for sequence, score in data_pairs:\n",
    "        sequence = sequence\n",
    "        score = round(float(score), 3)\n",
    "        emb1 = ge.generate_sequence_encodings(embedding_type, [sequence])[0].reshape(-1).tolist()\n",
    "        ohe = ge.generate_sequence_encodings(\"one_hot\", [sequence])[0].reshape(-1).tolist()\n",
    "        for binary in ohe:\n",
    "            emb1.append(binary)\n",
    "\n",
    "        representations.append(torch.tensor(emb1, dtype=torch.float32).cpu())\n",
    "        scores.append(score)\n",
    "        pbar.update(1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T13:56:55.443732Z",
     "start_time": "2025-03-12T13:56:52.224916Z"
    }
   },
   "cell_type": "code",
   "source": "print(np.array(representations)[1])",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7efd64745ca0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/conda/envs/MAP/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.2000000e+01 -2.3375000e+01  7.8437500e+00 -6.6250000e+00\n",
      "   3.3750000e+01 -1.1000000e+01 -9.0625000e+00 -1.6500000e+01\n",
      "  -2.0000000e+01  1.5937500e+01 -4.9062500e+00 -2.9875000e+01\n",
      "   1.0750000e+01  4.8250000e+01 -2.3500000e+01 -1.9531250e+00\n",
      "   3.8000000e+01 -4.4140625e-01 -2.5750000e+01  1.3812500e+01\n",
      "   4.9750000e+01  3.7750000e+01 -8.0000000e+00  3.9600000e+02\n",
      "  -5.3250000e+01 -3.3000000e+01 -2.3000000e+01  7.9500000e+01\n",
      "  -1.4843750e+00  4.2500000e+01 -2.8750000e+01 -5.5859375e-01\n",
      "   3.4750000e+01  1.6000000e+01  3.8750000e+01  6.8000000e+01\n",
      "  -5.8750000e+01 -3.1054688e-01  1.3125000e+01 -1.7750000e+01\n",
      "  -4.6000000e+01  2.4125000e+01  1.3937500e+01  1.9625000e+01\n",
      "  -2.0000000e+01 -2.3500000e+01  1.2312500e+01  4.8750000e+00\n",
      "  -5.1500000e+01  3.0875000e+01 -4.6750000e+01 -2.7750000e+01\n",
      "  -7.7734375e-01  7.8500000e+01 -1.0312500e+01 -4.7187500e+00\n",
      "  -1.1187500e+01 -2.3375000e+01 -1.0875000e+01 -1.7187500e-01\n",
      "  -1.4609375e+00  3.1500000e+01 -3.5000000e+01 -7.9062500e+00\n",
      "  -6.3125000e+00  3.2500000e+01 -2.3625000e+01  3.9000000e+01\n",
      "   2.3250000e+01 -6.3125000e+00 -2.6500000e+01  2.9125000e+01\n",
      "   1.3062500e+01 -1.2000000e+01  1.3937500e+01 -1.9125000e+01\n",
      "   2.4625000e+01 -6.0750000e+01  3.1375000e+01 -3.5250000e+01\n",
      "  -6.0546875e-01 -2.4500000e+01 -9.0500000e+01 -2.3375000e+01\n",
      "  -3.2750000e+01 -2.4250000e+01 -2.8375000e+01 -3.1093750e+00\n",
      "  -6.0000000e+01  1.7125000e+01 -1.4875000e+01  3.1250000e+01\n",
      "  -6.8437500e+00 -3.9250000e+01  2.3625000e+01 -6.3750000e+00\n",
      "   3.2500000e+01 -3.4000000e+01 -4.5000000e+01 -6.1500000e+01\n",
      "  -3.2750000e+01  1.7500000e+00 -2.7750000e+01  7.5937500e+00\n",
      "  -2.8250000e+01 -4.0000000e+01  4.3125000e+00 -1.8750000e+01\n",
      "   2.9875000e+01 -5.4250000e+01 -4.6250000e+01  6.9000000e+01\n",
      "  -3.1000000e+01 -4.8500000e+01  2.4625000e+01  9.8750000e+00\n",
      "   1.8906250e+00 -6.7187500e+00  1.2437500e+01 -1.9000000e+01\n",
      "  -1.8359375e+00 -4.9500000e+01 -1.4562500e+01  9.4375000e+00\n",
      "   2.5250000e+01  6.6500000e+01 -1.2937500e+01  1.2000000e+01\n",
      "  -2.7500000e+01 -3.3750000e+01 -2.5250000e+01 -3.5250000e+01\n",
      "  -2.8875000e+01  3.6718750e+00  8.9375000e+00  1.5750000e+01\n",
      "  -3.2000000e+01 -1.6375000e+01 -4.6500000e+01 -2.4375000e+01\n",
      "   3.8000000e+01 -3.7750000e+01 -7.0312500e+00  1.1000000e+01\n",
      "  -1.5812500e+01 -2.9000000e+01  2.2187500e+00  7.4375000e+00\n",
      "   3.8500000e+01  7.0312500e+00  2.1250000e+01 -2.5125000e+01\n",
      "  -8.4375000e+00  4.0000000e+01  1.5125000e+01 -7.5312500e+00\n",
      "   1.9000000e+01 -7.8125000e+00 -6.6250000e+00  8.9375000e+00\n",
      "   4.3125000e+00 -2.1750000e+01  2.4250000e+01  3.3500000e+01\n",
      "  -2.8437500e+00  6.2890625e-01 -8.9375000e+00 -4.1500000e+01\n",
      "  -1.5125000e+01 -5.2500000e+00 -5.2750000e+01 -2.4062500e+00\n",
      "  -4.5937500e+00 -3.4250000e+01  7.9375000e+00 -3.7250000e+01\n",
      "  -4.2750000e+01  5.0750000e+01 -7.1562500e+00 -1.6750000e+01\n",
      "  -1.4765625e+00 -5.2187500e+00 -3.1835938e-01  1.9875000e+01\n",
      "  -8.6875000e+00  1.4937500e+01 -6.9000000e+01 -3.5500000e+01\n",
      "  -8.9375000e+00  1.3437500e+01 -9.1250000e+00 -1.5750000e+01\n",
      "  -7.2500000e+00  2.2125000e+01  3.6750000e+01 -2.2500000e+01\n",
      "  -8.8750000e+00 -2.2375000e+01 -1.6000000e+01 -3.3750000e+01\n",
      "  -7.1500000e+01 -5.2750000e+01 -9.0625000e+00  1.2937500e+01\n",
      "  -2.8625000e+01  5.0250000e+01  4.3000000e+01  6.7187500e+00\n",
      "   6.2812500e+00 -1.2800000e+02 -1.4562500e+01  2.6625000e+01\n",
      "   1.5200000e+03 -4.8000000e+01  1.4125000e+01 -3.0625000e+01\n",
      "   9.5000000e+00 -3.7250000e+01 -1.8000000e+01  8.0078125e-01\n",
      "   2.2500000e+00 -7.1500000e+01  2.5000000e+01 -2.0625000e+01\n",
      "  -2.5625000e+00  1.1250000e+00 -4.9687500e+00 -6.1250000e+00\n",
      "  -2.7000000e+01 -5.3250000e+01 -5.9500000e+01  4.6250000e+01\n",
      "  -3.3750000e+01  1.0250000e+01  6.0937500e+00  1.1312500e+01\n",
      "   6.9500000e+01 -2.5125000e+01 -3.9500000e+01  8.9375000e+00\n",
      "  -4.0750000e+01  3.3250000e+01 -2.6125000e+01  5.4500000e+01\n",
      "   1.0312500e+01  2.2375000e+01  1.4562500e+01 -1.7750000e+01\n",
      "  -5.0250000e+01 -3.5500000e+01 -1.7089844e-02  5.0500000e+01\n",
      "  -4.8125000e+00  2.6500000e+01 -1.2375000e+01  4.4000000e+01\n",
      "  -1.8875000e+01  5.9000000e+01 -1.3687500e+01 -3.4500000e+01\n",
      "  -1.4625000e+01 -4.0250000e+01  6.2500000e+01  7.6562500e+00\n",
      "  -3.5000000e+01 -5.9500000e+01  1.8671875e+00 -4.2750000e+01\n",
      "  -1.7625000e+01  2.7375000e+01 -1.4187500e+01 -2.7250000e+01\n",
      "  -2.6625000e+01  4.7250000e+01 -2.0750000e+01 -1.8875000e+01\n",
      "  -5.5000000e+01 -5.9750000e+01 -2.1625000e+01 -1.1625000e+01\n",
      "   1.0062500e+01  2.5625000e+00 -3.7750000e+01  7.1562500e+00\n",
      "  -2.6250000e+01 -2.1125000e+01 -2.5500000e+01 -6.8000000e+01\n",
      "  -3.0875000e+01 -2.9125000e+01  6.5625000e+00  1.6125000e+01\n",
      "   5.5500000e+01  1.0625000e+01 -2.1500000e+01  3.3250000e+01\n",
      "  -3.5500000e+01  2.6000000e+01  2.7750000e+01  1.3515625e+00\n",
      "   1.7000000e+01 -3.4000000e+01  2.7375000e+01 -9.8125000e+00\n",
      "  -1.7656250e+00  2.0125000e+01  1.8000000e+01  3.2250000e+01\n",
      "  -1.2187500e+01 -7.1500000e+01 -1.2750000e+01  1.0187500e+01\n",
      "   4.4687500e+00  1.3312500e+01 -3.6250000e+01  3.7500000e+00\n",
      "  -2.3125000e+01  2.0125000e+01  2.1625000e+01  1.3437500e+01\n",
      "  -4.2500000e+01 -2.0625000e+01  1.8375000e+01  4.0250000e+01\n",
      "  -5.8125000e+00 -1.6601562e-02  3.0468750e+00  7.4062500e+00\n",
      "   6.0500000e+01 -3.9250000e+01 -1.9375000e+01 -1.9000000e+01\n",
      "  -1.9531250e+00  1.7000000e+01  2.0750000e+01  3.1625000e+01\n",
      "   2.8593750e+00 -1.7000000e+01 -6.9500000e+01 -3.3000000e+01\n",
      "   7.7187500e+00 -2.8750000e+00  1.2250000e+01 -3.5750000e+01\n",
      "  -1.4000000e+01 -1.1562500e+01 -4.7812500e+00 -2.7000000e+01\n",
      "   1.5812500e+01 -5.6750000e+01  7.1875000e+00  3.9750000e+01\n",
      "   8.4375000e+00  9.8125000e+00 -5.8437500e+00  2.8500000e+01\n",
      "   7.0937500e+00  9.0625000e+00  6.2500000e-01 -2.4687500e+00\n",
      "  -5.6500000e+01  2.0375000e+01  1.3187500e+01  1.5812500e+01\n",
      "   3.4750000e+01 -2.9625000e+01 -5.0000000e+00 -9.9375000e+00\n",
      "  -4.9500000e+01  3.3750000e+01  4.9500000e+01 -4.6250000e+01\n",
      "  -2.1406250e+00  1.3062500e+01 -1.5187500e+01 -2.0750000e+01\n",
      "  -1.3875000e+01  3.1875000e+01 -8.4375000e+00 -2.3750000e+01\n",
      "  -1.1125000e+01  5.1000000e+01  7.8750000e+00 -2.6875000e+00\n",
      "  -1.6500000e+01 -5.8250000e+01 -1.9375000e+01  1.4000000e+01\n",
      "   1.9625000e+01 -3.5500000e+01 -2.2031250e+00 -1.5156250e+00\n",
      "   1.4187500e+01  1.4687500e+01  2.1625000e+01 -3.6093750e+00\n",
      "  -4.1750000e+01  8.1000000e+01 -8.6000000e+01 -4.0500000e+01\n",
      "   3.0000000e+01  1.5875000e+01  6.0250000e+01 -1.3828125e+00\n",
      "   5.0750000e+01 -8.0625000e+00 -2.0500000e+01 -4.0250000e+01\n",
      "  -4.9250000e+01  3.7000000e+01  1.6250000e+01  3.5500000e+01\n",
      "   2.0000000e+01  1.3625000e+01 -1.7000000e+01  3.4250000e+01\n",
      "   4.0250000e+01 -1.2187500e+01 -3.1406250e+00  1.4687500e+01\n",
      "  -6.2500000e+01  3.0000000e+01  1.1125000e+01  4.3750000e+01\n",
      "   1.1812500e+01  3.7812500e+00 -2.7375000e+01  2.6000000e+01\n",
      "  -3.1375000e+01 -2.6875000e+00 -6.8437500e+00 -2.7000000e+01\n",
      "   3.7500000e+00  3.0500000e+01  3.2421875e-01  2.1875000e+01\n",
      "  -2.2750000e+01  1.4750000e+01  1.4562500e+01 -2.3906250e+00\n",
      "  -1.6375000e+01  1.9500000e+01 -5.7000000e+01 -4.0250000e+01\n",
      "   1.9750000e+01  1.5937500e+01 -4.1000000e+01 -2.2000000e+01\n",
      "   7.7500000e+00 -1.3000000e+01  2.7187500e+00  1.6500000e+01\n",
      "   3.8750000e+01 -5.5000000e+01 -3.5000000e+01 -2.8125000e+01\n",
      "  -1.0187500e+01 -2.9500000e+01 -1.9687500e+00 -1.7000000e+01\n",
      "  -4.9750000e+01 -5.8750000e+01 -4.9687500e+00  4.0312500e+00\n",
      "   9.5625000e+00 -7.7500000e+01 -3.3250000e+01 -2.2875000e+01\n",
      "  -1.1875000e+01  3.2750000e+01 -1.5000000e+01  6.0500000e+01\n",
      "   5.6500000e+01 -4.6250000e+01 -4.3250000e+01 -1.1562500e+01\n",
      "  -1.9250000e+01  1.9625000e+01 -2.2250000e+01 -2.6718750e+00\n",
      "  -4.7750000e+01  7.0000000e+01  2.2125000e+01  6.7000000e+01\n",
      "   2.3500000e+01 -1.0375000e+01 -6.5000000e+00  6.5625000e+00\n",
      "  -3.1250000e+01  4.4375000e+00 -1.1125000e+01 -2.7250000e+01\n",
      "  -9.3750000e+00  2.4625000e+01  5.1250000e+01  9.6875000e+00\n",
      "  -3.4000000e+01  1.2265625e+00  4.2000000e+01  3.8250000e+01\n",
      "  -3.7031250e+00  1.3500000e+01  2.3250000e+01 -7.3750000e+00\n",
      "  -3.9794922e-02  7.7812500e+00  1.9000000e+01 -7.7187500e+00\n",
      "   4.8750000e+00  6.9375000e+00 -3.6250000e+01 -5.9000000e+01\n",
      "  -3.1875000e+01  1.3687500e+01 -2.3375000e+01 -4.5750000e+01\n",
      "  -9.4375000e+00  3.3000000e+01  1.5562500e+01 -4.3000000e+01\n",
      "  -1.0078125e+00 -2.1250000e+00  2.8125000e+01 -2.0000000e+01\n",
      "  -1.7250000e+01  4.4500000e+01 -2.5750000e+01  1.4812500e+01\n",
      "  -2.5375000e+01  9.2000000e+01 -5.9250000e+01 -2.9250000e+01\n",
      "   1.0859375e+00 -4.7250000e+01 -4.3750000e+01 -3.0375000e+01\n",
      "  -2.0000000e+01 -3.1562500e+00 -4.4500000e+01  9.6875000e+00\n",
      "   3.3500000e+01 -1.9875000e+01  7.1250000e+00  3.1375000e+01\n",
      "  -8.0625000e+00 -1.9500000e+01 -4.3500000e+01 -4.3500000e+01\n",
      "  -7.2000000e+01  5.2500000e+01 -6.6000000e+01  2.9375000e+01\n",
      "  -1.2250000e+01 -2.4250000e+01 -2.5000000e+00  2.6500000e+01\n",
      "   1.2375000e+01 -3.3250000e+01 -6.3437500e+00  2.3000000e+01\n",
      "   1.9875000e+01  2.8375000e+01 -2.5250000e+01  3.2343750e+00\n",
      "  -2.8250000e+01  1.4000000e+01  5.2750000e+01  6.9062500e+00\n",
      "   2.0750000e+01  2.0500000e+01  3.9500000e+01 -3.1625000e+01\n",
      "  -3.4000000e+01  3.4000000e+01 -1.0625000e+01  7.0500000e+01\n",
      "  -4.8750000e+01 -2.4375000e+01  1.4453125e+00 -4.7000000e+01\n",
      "   1.0312500e+01  5.7000000e+01 -3.8250000e+01 -1.8625000e+01\n",
      "   5.8250000e+01 -1.0500000e+01 -2.3125000e+01 -8.2500000e+01\n",
      "   1.8125000e+00  1.5750000e+01 -4.4500000e+01  6.0000000e+01\n",
      "   1.9875000e+01  1.7375000e+01 -2.4000000e+01  3.8906250e+00\n",
      "  -3.3000000e+01 -2.8000000e+01  6.2000000e+01 -7.0312500e+00\n",
      "  -1.7500000e+01  1.6500000e+01  3.8000000e+01 -2.1000000e+01\n",
      "  -8.5000000e+01 -1.4000000e+01 -3.5000000e+01  7.9500000e+01\n",
      "   8.9062500e-01  8.6250000e+00 -1.9125000e+01 -4.7750000e+01\n",
      "  -9.4375000e+00  2.0125000e+01  4.3000000e+01  3.3000000e+01\n",
      "   8.8750000e+00  7.9500000e+01  1.6750000e+01  4.5000000e+01\n",
      "  -2.4125000e+01  6.8750000e+00 -3.3000000e+01  2.5000000e+01\n",
      "  -3.3250000e+01 -5.7812500e+00  7.4500000e+01 -3.2250000e+01\n",
      "  -3.6250000e+01  2.9000000e+01 -3.4000000e+01 -9.5000000e+00\n",
      "  -9.2000000e+01 -2.6125000e+01 -6.0000000e+01  1.6125000e+01\n",
      "   8.8750000e+00 -1.9500000e+01  3.5156250e+00  6.6000000e+01\n",
      "  -1.5781250e+00  3.5312500e+00 -3.0625000e+01  3.0125000e+01\n",
      "  -2.2875000e+01 -2.6250000e+01  2.4250000e+01  1.1250000e+01\n",
      "  -1.1687500e+01  2.0250000e+01  1.4687500e+01  1.9500000e+01\n",
      "  -1.9875000e+01 -1.0312500e+01 -1.2125000e+01 -3.2250000e+01\n",
      "   4.6000000e+01  2.3000000e+01 -3.6750000e+01 -3.5750000e+01\n",
      "   2.3000000e+01  6.1875000e+00  9.5000000e+00 -5.5500000e+01\n",
      "  -4.4500000e+01  6.3000000e+01 -3.4250000e+01  1.8375000e+01\n",
      "   1.8828125e+00 -4.0000000e+01 -1.6796875e+00 -2.3000000e+01\n",
      "  -2.7250000e+01  3.1000000e+01  2.8875000e+01 -2.1625000e+01\n",
      "   1.7750000e+01  6.6500000e+01  1.5750000e+01  1.8625000e+01\n",
      "  -1.9125000e+01  5.8437500e+00 -5.3500000e+01 -8.1250000e+00\n",
      "   2.3125000e+01 -2.8125000e+01  8.6250000e+00 -2.6750000e+01\n",
      "  -1.9625000e+01 -6.0750000e+01 -3.4000000e+01  7.7500000e+01\n",
      "   1.5437500e+01  3.1125000e+01 -5.4750000e+01 -6.0000000e+00\n",
      "   1.6953125e+00  1.4937500e+01 -2.6500000e+01 -1.8500000e+01\n",
      "   1.5781250e+00 -2.7125000e+01  1.5625000e+01 -2.8750000e+01\n",
      "   3.6750000e+01  5.6500000e+01 -7.5000000e+01  1.5812500e+01\n",
      "   1.9250000e+01 -3.8000000e+01 -1.0687500e+01 -1.3437500e+01\n",
      "  -4.6000000e+01 -1.3062500e+01 -3.4218750e+00  4.3000000e+01\n",
      "  -1.1062500e+01 -1.5781250e+00 -7.7812500e+00 -4.5250000e+01\n",
      "  -7.7500000e+01  4.5166016e-02 -3.2500000e+00 -3.7750000e+01\n",
      "   2.4200000e+02 -7.1562500e+00 -2.0500000e+01 -4.0250000e+01\n",
      "   1.1812500e+01 -1.8750000e+01 -3.4250000e+01  2.7343750e+00\n",
      "   1.6000000e+01 -3.9750000e+01 -9.8125000e+00 -4.7000000e+01\n",
      "  -2.0250000e+01 -3.4000000e+01 -1.1953125e+00  5.1250000e+01\n",
      "  -8.5625000e+00  7.2500000e+01 -2.8875000e+01  4.1000000e+01\n",
      "  -4.6000000e+01 -6.1562500e+00 -2.4375000e+00 -4.1500000e+01\n",
      "   4.5000000e+01 -9.6000000e+01 -3.6500000e+01  3.4500000e+01\n",
      "  -6.7000000e+01  2.7125000e+01 -4.3500000e+01 -4.4250000e+01\n",
      "  -8.7000000e+01  2.9250000e+01  6.5000000e+00  4.6000000e+01\n",
      "  -3.3750000e+01 -2.8000000e+01  5.7187500e+00 -3.4000000e+01\n",
      "  -4.8000000e+01 -1.7125000e+01 -4.0000000e+01 -4.2000000e+01\n",
      "  -3.7500000e+01  4.8500000e+01 -1.1750000e+01  2.0750000e+01\n",
      "   1.6000000e+01 -4.1000000e+01  1.6875000e+01  2.4125000e+01\n",
      "  -2.0000000e+01 -2.7968750e+00 -7.8437500e+00  1.8375000e+01\n",
      "   2.0468750e+00  1.8500000e+01  3.0125000e+01  1.2500000e+02\n",
      "  -2.4625000e+01 -1.7250000e+01 -1.3359375e+00  1.3812500e+01\n",
      "  -4.8000000e+01 -4.7187500e+00 -5.1500000e+01 -6.3476562e-02\n",
      "   3.5500000e+01 -4.4750000e+01  2.0250000e+01 -1.4375000e+01\n",
      "  -1.3375000e+01  3.4000000e+01 -1.4937500e+01  1.6375000e+01\n",
      "  -5.8203125e-01  5.0937500e+00 -2.4625000e+01 -3.1375000e+01\n",
      "  -7.2500000e+00 -1.8515625e+00 -1.2062500e+01  6.5312500e+00\n",
      "   1.0000000e+01  2.8875000e+01 -1.5812500e+01 -2.1250000e+01\n",
      "  -2.8500000e+01 -9.8828125e-01  1.7625000e+01  9.0000000e+00\n",
      "  -1.6750000e+01  1.5750000e+01  1.3437500e+01  2.8375000e+01\n",
      "  -5.1000000e+01 -2.3500000e+01 -3.3000000e+01 -6.5000000e+00\n",
      "  -2.3250000e+01 -1.1687500e+01 -8.8000000e+01 -3.0125000e+01\n",
      "  -1.4312500e+01 -6.3125000e+00 -6.3250000e+01 -2.6750000e+01\n",
      "   7.0500000e+01  2.9500000e+01  1.4437500e+01 -1.3375000e+01\n",
      "   4.8000000e+01 -3.5500000e+01  1.7500000e+01 -3.4000000e+01\n",
      "  -1.0950000e+02 -6.5312500e+00 -2.7750000e+01  6.7500000e+01\n",
      "  -2.1750000e+01  4.6750000e+01 -1.3937500e+01  4.7250000e+01\n",
      "   2.8375000e+01 -3.6500000e+01 -3.7500000e+01 -1.1500000e+01\n",
      "   1.2109375e+00 -6.9500000e+01 -2.2000000e+01 -4.6000000e+01\n",
      "  -1.1687500e+01  4.0000000e+01  6.8125000e+00  1.5625000e+01\n",
      "   3.4000000e+01 -1.0937500e+01 -6.1500000e+01  1.5234375e+00\n",
      "  -4.9750000e+01 -2.7500000e+01  8.0625000e+00 -6.5000000e+01\n",
      "  -2.7000000e+01  5.7500000e+01 -3.4500000e+01  2.5937500e+00\n",
      "   1.5375000e+01 -2.3375000e+01  7.7812500e+00 -4.2750000e+01\n",
      "   2.8375000e+01 -3.1125000e+01  4.4000000e+01 -1.5750000e+01\n",
      "  -1.8500000e+01 -7.5500000e+01 -1.9000000e+01 -5.2187500e+00\n",
      "   1.8984375e+00  3.5500000e+01  6.5000000e+00 -4.2750000e+01\n",
      "   3.4250000e+01  1.1187500e+01 -2.9875000e+01 -7.1500000e+01\n",
      "   4.8125000e+00 -1.8500000e+01  3.5937500e+00  3.2500000e+00\n",
      "  -3.1875000e+01  1.8875000e+01  3.3250000e+01 -2.4500000e+01\n",
      "   3.0125000e+01 -1.8875000e+01 -7.5937500e+00  3.9000000e+01\n",
      "  -8.6250000e+00  7.6562500e+00  2.4750000e+01  7.0937500e+00\n",
      "   8.1250000e+00  8.5000000e+00 -5.1250000e+01  1.1000000e+01\n",
      "   1.7750000e+01 -5.6562500e+00 -5.7187500e+00 -2.7125000e+01\n",
      "   1.5687500e+01 -9.6500000e+01  3.6000000e+01  3.3000000e+01\n",
      "   9.9375000e+00  1.3562500e+01 -3.7750000e+01 -4.8250000e+01\n",
      "   1.8125000e+01  2.3250000e+01 -2.0312500e+00 -1.2625000e+01\n",
      "   1.2625000e+01 -1.9125000e+01 -1.7875000e+01  1.0500000e+01\n",
      "   7.3000000e+01  2.1250000e+00 -3.9250000e+01  4.7000000e+01\n",
      "  -5.4250000e+01  2.9125000e+01  1.3062500e+01 -6.8500000e+01\n",
      "  -3.9500000e+01 -6.5000000e+01  1.2187500e+01  2.2875000e+01\n",
      "   4.7500000e+01 -1.4000000e+01 -1.5187500e+01 -2.8625000e+01\n",
      "  -9.5500000e+01  2.8125000e+01  1.8125000e+00 -3.6250000e+01\n",
      "   1.8875000e+01 -8.2500000e+00 -1.4000000e+01 -8.1250000e+00\n",
      "   8.8750000e+00  4.8000000e+01 -5.3250000e+01 -3.9687500e+00]]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T13:57:01.966878Z",
     "start_time": "2025-03-12T13:56:58.400328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "split_ratio = (0.75, 0.15, 0.10)\n",
    "split_train = int(len(representations) * split_ratio[0])\n",
    "split_valid = int(len(representations) * (split_ratio[1] + split_ratio[0]))\n",
    "\n",
    "# if isinstance(representations[0], torch.Tensor):\n",
    "x_train = torch.stack([x.reshape(-1).to(dtype=torch.float32) for x in representations[0:split_train]])\n",
    "x_valid = torch.stack([x.reshape(-1).to(dtype=torch.float32) for x in representations[split_train:split_valid]])\n",
    "x_test = torch.stack([x.reshape(-1).to(dtype=torch.float32) for x in representations[split_valid:]])\n",
    "\n",
    "y_train = torch.stack([torch.tensor(y, dtype=torch.float32) for y in scores[0:split_train]])\n",
    "y_valid = torch.stack([torch.tensor(y, dtype=torch.float32) for y in scores[split_train: split_valid]])\n",
    "y_test = torch.stack([torch.tensor(y, dtype=torch.float32) for y in scores[split_valid:]])\n",
    "\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=1000, shuffle=False)\n",
    "\n",
    "test_data = TensorDataset(x_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=1000, shuffle=False)\n",
    "\n",
    "valid_data = TensorDataset(x_valid, y_valid)\n",
    "valid_loader = DataLoader(valid_data, batch_size=1000, shuffle=False)\n",
    "\n",
    "input_len = len(x_train[0])\n",
    "print(input_len)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gc\n",
    "del representations\n",
    "del scores\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T13:57:05.953284Z",
     "start_time": "2025-03-12T13:57:05.950821Z"
    }
   },
   "source": [
    "class FFNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FFNet, self).__init__()\n",
    "\n",
    "        hidden_size = int(input_len / 4)\n",
    "\n",
    "        self.fc1 = nn.Linear(input_len, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        # self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        # self.fc5 = nn.Linear(hidden_size, hidden_size)\n",
    "        # self.fc6 = nn.Linear(hidden_size, hidden_size)\n",
    "        # self.fc7 = nn.Linear(hidden_size, hidden_size)\n",
    "        # self.fc8 = nn.Linear(hidden_size, hidden_size)\n",
    "        # self.fc9 = nn.Linear(hidden_size, hidden_size)\n",
    "        # self.fc10 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)\n",
    "        # leaky = nn.LeakyReLU(0.1)\n",
    "        # self.output_layer = leaky(self.output_layer(input_len, 1))\n",
    "        # self.output_layer = nn.Linear(input_len, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        # x = torch.relu(self.fc4(x))\n",
    "        # x = torch.relu(self.fc5(x))\n",
    "        # x = torch.relu(self.fc6(x))\n",
    "        # x = torch.relu(self.fc7(x))\n",
    "        # x = torch.relu(self.fc8(x))\n",
    "        # x = torch.relu(self.fc9(x))\n",
    "        # x = torch.relu(self.fc10(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# x = torch.relu(self.fc4(x))\n",
    "# x = torch.relu(self.fc5(x))\n",
    "# x = torch.relu(self.fc6(x))\n",
    "# x = torch.relu(self.fc7(x))\n",
    "# x = torch.relu(self.fc8(x))\n",
    "# x = torch.relu(self.fc9(x))\n",
    "# x = torch.relu(self.fc10(x))\n",
    "# x = torch.relu(self.fc11(x))\n",
    "# x = torch.relu(self.fc12(x))\n",
    "# x = torch.relu(self.fc13(x))\n",
    "# x = torch.relu(self.fc14(x))\n",
    "# x = torch.relu(self.fc15(x))\n",
    "# x = torch.relu(self.fc16(x))\n",
    "# x = torch.relu(self.fc17(x))\n",
    "# x = torch.relu(self.fc18(x))\n",
    "# x = torch.relu(self.fc19(x))\n",
    "# x = torch.relu(self.fc20(x))\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T14:07:19.402882Z",
     "start_time": "2025-03-12T14:07:19.397129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"Creating up the neural net + log\"\"\"\n",
    "gt_model = FFNet()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gt_model.to(device, dtype=torch.float32)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "learning_rate = 0.000003\n",
    "momentum = 0.1\n",
    "optim = \"ADAM\"\n",
    "if optim == \"SGD\":\n",
    "    optimizer = torch.optim.SGD(gt_model.parameters(), lr=learning_rate, momentum=momentum)  #, momentum=momentum)\n",
    "else:\n",
    "    optimizer = torch.optim.Adam(gt_model.parameters(), lr=learning_rate / 100)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T14:07:20.766155Z",
     "start_time": "2025-03-12T14:07:20.764017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_set = \"HIS7_YEAST_Pokusaeva_2019\"\n",
    "embedding_type = \"esmc_300m\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Track Training Metrics\"\"\"\n",
    "time_stamp = datetime.datetime.now().strftime(\"%Y_%m_%d-%H_%M\")\n",
    "outfile = f\"../Models/Logs/{time_stamp}.txt\"\n",
    "with open(outfile, \"w\") as f:\n",
    "    f.write(\n",
    "        f\"Dataset: {data_set}, Encoding-Method: {embedding_type}, Optimizer: {optim}, criterion: {criterion},learning_rate: {learning_rate}, momentum: {momentum} \\n\")\n",
    "    f.write(f\"Model: \\n {gt_model} \\n\")\n",
    "    f.write(\"\\n\")"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T14:31:25.243570Z",
     "start_time": "2025-03-12T14:07:22.353573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.cuda.empty_cache()\n",
    "n_epochs = 500\n",
    "diplay_every_n_epochs = 1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    gt_model.train()\n",
    "    running_loss = 0.0\n",
    "    tr_y_pred = []\n",
    "    tr_y_true = []\n",
    "\n",
    "    best_val = 1\n",
    "    best_model = None\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for tr_inputs, tr_labels in train_loader:\n",
    "        # Move data to the same device as the model\n",
    "        tr_inputs = tr_inputs.to(device)\n",
    "        tr_labels = tr_labels.to(device)\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "        tr_outputs = gt_model(tr_inputs)  # Forward pass\n",
    "\n",
    "        loss = criterion(tr_outputs, tr_labels)\n",
    "        running_loss += loss.item()\n",
    "        for i, result in enumerate(tr_outputs):\n",
    "            result_ = result.cpu().detach().numpy().tolist()[0]\n",
    "            tr_y_pred.append(result_)\n",
    "        for i, label in enumerate(tr_labels):\n",
    "            label_ = label.cpu().detach().numpy().tolist()\n",
    "            tr_y_true.append(label_)\n",
    "        loss.backward()\n",
    "        # optimizer.step()\n",
    "    optimizer.step()\n",
    "    r2_train = round(r2_score(tr_y_true, tr_y_pred),3)\n",
    "    # scheduler.step()\n",
    "\n",
    "    gt_model.eval()\n",
    "    with torch.no_grad():\n",
    "        te_loss = 0.0\n",
    "        running_va_loss = 0.0\n",
    "\n",
    "        va_y_pred = []\n",
    "        va_y_true = []\n",
    "\n",
    "        for va_input, va_label in valid_loader:\n",
    "            # Move data to the same device as the model\n",
    "            va_input = va_input.to(device)\n",
    "            va_labels = va_label.to(device)\n",
    "            va_outputs = gt_model(va_input) # Forward pass\n",
    "            va_loss = criterion(va_outputs.cpu(), va_label.cpu())\n",
    "            running_va_loss += va_loss.item()\n",
    "            for i, result in enumerate(va_outputs):\n",
    "                result_ = result.cpu().detach().numpy().tolist()[0]\n",
    "                va_y_pred.append(result_)\n",
    "            for i, label in enumerate(va_labels):\n",
    "                label_ = label.cpu().detach().numpy().tolist()\n",
    "                va_y_true.append(label_)\n",
    "\n",
    "\n",
    "        r2_val = round(r2_score(va_y_true, va_y_pred),3)\n",
    "        tr_RMSE = math.sqrt(running_loss / len(train_loader))\n",
    "        va_RMSE = math.sqrt(running_va_loss / len(valid_loader))\n",
    "\n",
    "    with open(outfile, \"a\") as f:\n",
    "        f.write(\n",
    "            f'[Epoch {epoch + 1}/{n_epochs}, t: {round(time.time() - start_time, 2)}s]:tLoss: {running_loss / len(train_loader):.4f}, tR2: {r2_train}, tRMSE: {tr_RMSE:.4f}, '\n",
    "            f'vLoss: {running_va_loss / len(valid_loader):.4f}, vR2: {r2_val}, vRMSE: {va_RMSE:.4f} \\n')  #,\n",
    "\n",
    "    if (epoch + 1) % diplay_every_n_epochs == 0:\n",
    "        print(\n",
    "            f'[Epoch {epoch + 1}/{n_epochs}, t: {round(time.time() - start_time, 2)}s]:tLoss: {running_loss / len(train_loader):.4f}, tR2: {r2_train}, tRMSE: {tr_RMSE:.4f}, '\n",
    "            f'vLoss: {running_va_loss / len(valid_loader):.4f}, vR2: {r2_val}, vRMSE: {va_RMSE:.4f}')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/500, t: 9.28s]:tLoss: 0.5962, tR2: -2.057, tRMSE: 0.7722, vLoss: 0.5978, vR2: -2.062, vRMSE: 0.7732\n",
      "[Epoch 2/500, t: 9.44s]:tLoss: 0.5944, tR2: -2.047, tRMSE: 0.7710, vLoss: 0.5960, vR2: -2.053, vRMSE: 0.7720\n",
      "[Epoch 3/500, t: 9.57s]:tLoss: 0.5926, tR2: -2.038, tRMSE: 0.7698, vLoss: 0.5941, vR2: -2.044, vRMSE: 0.7708\n",
      "[Epoch 4/500, t: 9.65s]:tLoss: 0.5907, tR2: -2.029, tRMSE: 0.7686, vLoss: 0.5923, vR2: -2.035, vRMSE: 0.7696\n",
      "[Epoch 5/500, t: 9.41s]:tLoss: 0.5889, tR2: -2.02, tRMSE: 0.7674, vLoss: 0.5905, vR2: -2.026, vRMSE: 0.7684\n",
      "[Epoch 6/500, t: 9.35s]:tLoss: 0.5871, tR2: -2.011, tRMSE: 0.7662, vLoss: 0.5886, vR2: -2.016, vRMSE: 0.7672\n",
      "[Epoch 7/500, t: 9.62s]:tLoss: 0.5853, tR2: -2.002, tRMSE: 0.7650, vLoss: 0.5868, vR2: -2.007, vRMSE: 0.7660\n",
      "[Epoch 8/500, t: 9.31s]:tLoss: 0.5835, tR2: -1.993, tRMSE: 0.7638, vLoss: 0.5850, vR2: -1.998, vRMSE: 0.7648\n",
      "[Epoch 9/500, t: 9.35s]:tLoss: 0.5816, tR2: -1.983, tRMSE: 0.7627, vLoss: 0.5832, vR2: -1.989, vRMSE: 0.7637\n",
      "[Epoch 10/500, t: 9.33s]:tLoss: 0.5798, tR2: -1.974, tRMSE: 0.7615, vLoss: 0.5814, vR2: -1.98, vRMSE: 0.7625\n",
      "[Epoch 11/500, t: 9.47s]:tLoss: 0.5780, tR2: -1.965, tRMSE: 0.7603, vLoss: 0.5796, vR2: -1.971, vRMSE: 0.7613\n",
      "[Epoch 12/500, t: 9.39s]:tLoss: 0.5763, tR2: -1.956, tRMSE: 0.7591, vLoss: 0.5778, vR2: -1.962, vRMSE: 0.7601\n",
      "[Epoch 13/500, t: 9.73s]:tLoss: 0.5745, tR2: -1.947, tRMSE: 0.7579, vLoss: 0.5760, vR2: -1.953, vRMSE: 0.7589\n",
      "[Epoch 14/500, t: 9.44s]:tLoss: 0.5727, tR2: -1.939, tRMSE: 0.7568, vLoss: 0.5742, vR2: -1.944, vRMSE: 0.7578\n",
      "[Epoch 15/500, t: 9.42s]:tLoss: 0.5709, tR2: -1.93, tRMSE: 0.7556, vLoss: 0.5724, vR2: -1.935, vRMSE: 0.7566\n",
      "[Epoch 16/500, t: 9.36s]:tLoss: 0.5691, tR2: -1.921, tRMSE: 0.7544, vLoss: 0.5706, vR2: -1.927, vRMSE: 0.7554\n",
      "[Epoch 17/500, t: 9.67s]:tLoss: 0.5674, tR2: -1.912, tRMSE: 0.7532, vLoss: 0.5689, vR2: -1.918, vRMSE: 0.7542\n",
      "[Epoch 18/500, t: 9.61s]:tLoss: 0.5656, tR2: -1.903, tRMSE: 0.7521, vLoss: 0.5671, vR2: -1.909, vRMSE: 0.7531\n",
      "[Epoch 19/500, t: 9.62s]:tLoss: 0.5638, tR2: -1.894, tRMSE: 0.7509, vLoss: 0.5653, vR2: -1.9, vRMSE: 0.7519\n",
      "[Epoch 20/500, t: 9.65s]:tLoss: 0.5621, tR2: -1.885, tRMSE: 0.7497, vLoss: 0.5636, vR2: -1.891, vRMSE: 0.7507\n",
      "[Epoch 21/500, t: 9.67s]:tLoss: 0.5603, tR2: -1.877, tRMSE: 0.7485, vLoss: 0.5618, vR2: -1.882, vRMSE: 0.7495\n",
      "[Epoch 22/500, t: 9.79s]:tLoss: 0.5586, tR2: -1.868, tRMSE: 0.7474, vLoss: 0.5601, vR2: -1.874, vRMSE: 0.7484\n",
      "[Epoch 23/500, t: 9.55s]:tLoss: 0.5568, tR2: -1.859, tRMSE: 0.7462, vLoss: 0.5583, vR2: -1.865, vRMSE: 0.7472\n",
      "[Epoch 24/500, t: 9.59s]:tLoss: 0.5551, tR2: -1.85, tRMSE: 0.7450, vLoss: 0.5566, vR2: -1.856, vRMSE: 0.7460\n",
      "[Epoch 25/500, t: 9.57s]:tLoss: 0.5534, tR2: -1.842, tRMSE: 0.7439, vLoss: 0.5549, vR2: -1.848, vRMSE: 0.7449\n",
      "[Epoch 26/500, t: 9.43s]:tLoss: 0.5516, tR2: -1.833, tRMSE: 0.7427, vLoss: 0.5531, vR2: -1.839, vRMSE: 0.7437\n",
      "[Epoch 27/500, t: 9.4s]:tLoss: 0.5499, tR2: -1.824, tRMSE: 0.7416, vLoss: 0.5514, vR2: -1.83, vRMSE: 0.7426\n",
      "[Epoch 28/500, t: 9.56s]:tLoss: 0.5482, tR2: -1.816, tRMSE: 0.7404, vLoss: 0.5497, vR2: -1.822, vRMSE: 0.7414\n",
      "[Epoch 29/500, t: 9.58s]:tLoss: 0.5465, tR2: -1.807, tRMSE: 0.7392, vLoss: 0.5480, vR2: -1.813, vRMSE: 0.7403\n",
      "[Epoch 30/500, t: 9.59s]:tLoss: 0.5448, tR2: -1.799, tRMSE: 0.7381, vLoss: 0.5463, vR2: -1.805, vRMSE: 0.7391\n",
      "[Epoch 31/500, t: 9.76s]:tLoss: 0.5431, tR2: -1.79, tRMSE: 0.7369, vLoss: 0.5446, vR2: -1.796, vRMSE: 0.7379\n",
      "[Epoch 32/500, t: 9.58s]:tLoss: 0.5414, tR2: -1.782, tRMSE: 0.7358, vLoss: 0.5429, vR2: -1.788, vRMSE: 0.7368\n",
      "[Epoch 33/500, t: 9.58s]:tLoss: 0.5397, tR2: -1.773, tRMSE: 0.7346, vLoss: 0.5412, vR2: -1.779, vRMSE: 0.7356\n",
      "[Epoch 34/500, t: 9.41s]:tLoss: 0.5380, tR2: -1.765, tRMSE: 0.7335, vLoss: 0.5395, vR2: -1.771, vRMSE: 0.7345\n",
      "[Epoch 35/500, t: 9.58s]:tLoss: 0.5363, tR2: -1.756, tRMSE: 0.7323, vLoss: 0.5378, vR2: -1.762, vRMSE: 0.7334\n",
      "[Epoch 36/500, t: 9.57s]:tLoss: 0.5347, tR2: -1.748, tRMSE: 0.7312, vLoss: 0.5361, vR2: -1.754, vRMSE: 0.7322\n",
      "[Epoch 37/500, t: 9.6s]:tLoss: 0.5330, tR2: -1.74, tRMSE: 0.7301, vLoss: 0.5345, vR2: -1.746, vRMSE: 0.7311\n",
      "[Epoch 38/500, t: 9.37s]:tLoss: 0.5313, tR2: -1.731, tRMSE: 0.7289, vLoss: 0.5328, vR2: -1.737, vRMSE: 0.7299\n",
      "[Epoch 39/500, t: 9.53s]:tLoss: 0.5297, tR2: -1.723, tRMSE: 0.7278, vLoss: 0.5312, vR2: -1.729, vRMSE: 0.7288\n",
      "[Epoch 40/500, t: 9.64s]:tLoss: 0.5280, tR2: -1.715, tRMSE: 0.7267, vLoss: 0.5295, vR2: -1.721, vRMSE: 0.7277\n",
      "[Epoch 41/500, t: 9.43s]:tLoss: 0.5264, tR2: -1.707, tRMSE: 0.7255, vLoss: 0.5279, vR2: -1.713, vRMSE: 0.7266\n",
      "[Epoch 42/500, t: 9.57s]:tLoss: 0.5248, tR2: -1.698, tRMSE: 0.7244, vLoss: 0.5262, vR2: -1.705, vRMSE: 0.7254\n",
      "[Epoch 43/500, t: 9.57s]:tLoss: 0.5231, tR2: -1.69, tRMSE: 0.7233, vLoss: 0.5246, vR2: -1.696, vRMSE: 0.7243\n",
      "[Epoch 44/500, t: 9.63s]:tLoss: 0.5215, tR2: -1.682, tRMSE: 0.7222, vLoss: 0.5230, vR2: -1.688, vRMSE: 0.7232\n",
      "[Epoch 45/500, t: 9.58s]:tLoss: 0.5199, tR2: -1.674, tRMSE: 0.7210, vLoss: 0.5214, vR2: -1.68, vRMSE: 0.7221\n",
      "[Epoch 46/500, t: 9.56s]:tLoss: 0.5183, tR2: -1.666, tRMSE: 0.7199, vLoss: 0.5198, vR2: -1.672, vRMSE: 0.7209\n",
      "[Epoch 47/500, t: 9.37s]:tLoss: 0.5167, tR2: -1.658, tRMSE: 0.7188, vLoss: 0.5182, vR2: -1.664, vRMSE: 0.7198\n",
      "[Epoch 48/500, t: 9.82s]:tLoss: 0.5151, tR2: -1.65, tRMSE: 0.7177, vLoss: 0.5166, vR2: -1.656, vRMSE: 0.7187\n",
      "[Epoch 49/500, t: 9.45s]:tLoss: 0.5135, tR2: -1.642, tRMSE: 0.7166, vLoss: 0.5150, vR2: -1.649, vRMSE: 0.7177\n",
      "[Epoch 50/500, t: 9.32s]:tLoss: 0.5120, tR2: -1.634, tRMSE: 0.7155, vLoss: 0.5135, vR2: -1.641, vRMSE: 0.7166\n",
      "[Epoch 51/500, t: 9.59s]:tLoss: 0.5104, tR2: -1.627, tRMSE: 0.7144, vLoss: 0.5119, vR2: -1.633, vRMSE: 0.7155\n",
      "[Epoch 52/500, t: 9.3s]:tLoss: 0.5089, tR2: -1.619, tRMSE: 0.7134, vLoss: 0.5104, vR2: -1.625, vRMSE: 0.7144\n",
      "[Epoch 53/500, t: 9.35s]:tLoss: 0.5073, tR2: -1.611, tRMSE: 0.7123, vLoss: 0.5088, vR2: -1.618, vRMSE: 0.7133\n",
      "[Epoch 54/500, t: 9.45s]:tLoss: 0.5058, tR2: -1.603, tRMSE: 0.7112, vLoss: 0.5073, vR2: -1.61, vRMSE: 0.7122\n",
      "[Epoch 55/500, t: 9.39s]:tLoss: 0.5043, tR2: -1.596, tRMSE: 0.7101, vLoss: 0.5058, vR2: -1.602, vRMSE: 0.7112\n",
      "[Epoch 56/500, t: 9.39s]:tLoss: 0.5027, tR2: -1.588, tRMSE: 0.7090, vLoss: 0.5042, vR2: -1.595, vRMSE: 0.7101\n",
      "[Epoch 57/500, t: 9.78s]:tLoss: 0.5012, tR2: -1.58, tRMSE: 0.7080, vLoss: 0.5027, vR2: -1.587, vRMSE: 0.7090\n",
      "[Epoch 58/500, t: 9.36s]:tLoss: 0.4997, tR2: -1.573, tRMSE: 0.7069, vLoss: 0.5012, vR2: -1.579, vRMSE: 0.7079\n",
      "[Epoch 59/500, t: 9.4s]:tLoss: 0.4982, tR2: -1.565, tRMSE: 0.7058, vLoss: 0.4997, vR2: -1.572, vRMSE: 0.7069\n",
      "[Epoch 60/500, t: 9.39s]:tLoss: 0.4967, tR2: -1.558, tRMSE: 0.7048, vLoss: 0.4982, vR2: -1.564, vRMSE: 0.7058\n",
      "[Epoch 61/500, t: 9.4s]:tLoss: 0.4952, tR2: -1.55, tRMSE: 0.7037, vLoss: 0.4967, vR2: -1.557, vRMSE: 0.7048\n",
      "[Epoch 62/500, t: 9.44s]:tLoss: 0.4937, tR2: -1.543, tRMSE: 0.7026, vLoss: 0.4952, vR2: -1.549, vRMSE: 0.7037\n",
      "[Epoch 63/500, t: 9.44s]:tLoss: 0.4922, tR2: -1.535, tRMSE: 0.7016, vLoss: 0.4937, vR2: -1.542, vRMSE: 0.7026\n",
      "[Epoch 64/500, t: 9.41s]:tLoss: 0.4907, tR2: -1.528, tRMSE: 0.7005, vLoss: 0.4922, vR2: -1.534, vRMSE: 0.7016\n",
      "[Epoch 65/500, t: 9.46s]:tLoss: 0.4893, tR2: -1.521, tRMSE: 0.6995, vLoss: 0.4907, vR2: -1.527, vRMSE: 0.7005\n",
      "[Epoch 66/500, t: 9.31s]:tLoss: 0.4878, tR2: -1.513, tRMSE: 0.6984, vLoss: 0.4893, vR2: -1.52, vRMSE: 0.6995\n",
      "[Epoch 67/500, t: 9.21s]:tLoss: 0.4863, tR2: -1.506, tRMSE: 0.6974, vLoss: 0.4878, vR2: -1.512, vRMSE: 0.6984\n",
      "[Epoch 68/500, t: 9.36s]:tLoss: 0.4849, tR2: -1.499, tRMSE: 0.6963, vLoss: 0.4863, vR2: -1.505, vRMSE: 0.6974\n",
      "[Epoch 69/500, t: 9.4s]:tLoss: 0.4834, tR2: -1.491, tRMSE: 0.6953, vLoss: 0.4849, vR2: -1.498, vRMSE: 0.6963\n",
      "[Epoch 70/500, t: 9.4s]:tLoss: 0.4820, tR2: -1.484, tRMSE: 0.6942, vLoss: 0.4834, vR2: -1.491, vRMSE: 0.6953\n",
      "[Epoch 71/500, t: 9.38s]:tLoss: 0.4805, tR2: -1.477, tRMSE: 0.6932, vLoss: 0.4820, vR2: -1.484, vRMSE: 0.6943\n",
      "[Epoch 72/500, t: 9.37s]:tLoss: 0.4791, tR2: -1.47, tRMSE: 0.6922, vLoss: 0.4806, vR2: -1.476, vRMSE: 0.6933\n",
      "[Epoch 73/500, t: 9.29s]:tLoss: 0.4777, tR2: -1.463, tRMSE: 0.6912, vLoss: 0.4792, vR2: -1.469, vRMSE: 0.6922\n",
      "[Epoch 74/500, t: 9.52s]:tLoss: 0.4763, tR2: -1.456, tRMSE: 0.6902, vLoss: 0.4778, vR2: -1.462, vRMSE: 0.6912\n",
      "[Epoch 75/500, t: 9.37s]:tLoss: 0.4749, tR2: -1.449, tRMSE: 0.6891, vLoss: 0.4764, vR2: -1.455, vRMSE: 0.6902\n",
      "[Epoch 76/500, t: 9.37s]:tLoss: 0.4735, tR2: -1.442, tRMSE: 0.6881, vLoss: 0.4750, vR2: -1.448, vRMSE: 0.6892\n",
      "[Epoch 77/500, t: 9.34s]:tLoss: 0.4721, tR2: -1.435, tRMSE: 0.6871, vLoss: 0.4736, vR2: -1.442, vRMSE: 0.6882\n",
      "[Epoch 78/500, t: 9.34s]:tLoss: 0.4708, tR2: -1.428, tRMSE: 0.6861, vLoss: 0.4722, vR2: -1.435, vRMSE: 0.6872\n",
      "[Epoch 79/500, t: 9.38s]:tLoss: 0.4694, tR2: -1.421, tRMSE: 0.6851, vLoss: 0.4709, vR2: -1.428, vRMSE: 0.6862\n",
      "[Epoch 80/500, t: 9.17s]:tLoss: 0.4680, tR2: -1.414, tRMSE: 0.6841, vLoss: 0.4695, vR2: -1.421, vRMSE: 0.6852\n",
      "[Epoch 81/500, t: 9.16s]:tLoss: 0.4666, tR2: -1.407, tRMSE: 0.6831, vLoss: 0.4681, vR2: -1.414, vRMSE: 0.6842\n",
      "[Epoch 82/500, t: 9.25s]:tLoss: 0.4653, tR2: -1.4, tRMSE: 0.6821, vLoss: 0.4668, vR2: -1.407, vRMSE: 0.6832\n",
      "[Epoch 83/500, t: 9.34s]:tLoss: 0.4639, tR2: -1.394, tRMSE: 0.6811, vLoss: 0.4654, vR2: -1.401, vRMSE: 0.6822\n",
      "[Epoch 84/500, t: 9.17s]:tLoss: 0.4626, tR2: -1.387, tRMSE: 0.6801, vLoss: 0.4641, vR2: -1.394, vRMSE: 0.6812\n",
      "[Epoch 85/500, t: 9.19s]:tLoss: 0.4612, tR2: -1.38, tRMSE: 0.6791, vLoss: 0.4627, vR2: -1.387, vRMSE: 0.6802\n",
      "[Epoch 86/500, t: 9.33s]:tLoss: 0.4599, tR2: -1.374, tRMSE: 0.6782, vLoss: 0.4614, vR2: -1.38, vRMSE: 0.6792\n",
      "[Epoch 87/500, t: 9.4s]:tLoss: 0.4586, tR2: -1.367, tRMSE: 0.6772, vLoss: 0.4600, vR2: -1.374, vRMSE: 0.6783\n",
      "[Epoch 88/500, t: 9.41s]:tLoss: 0.4572, tR2: -1.36, tRMSE: 0.6762, vLoss: 0.4587, vR2: -1.367, vRMSE: 0.6773\n",
      "[Epoch 89/500, t: 9.37s]:tLoss: 0.4559, tR2: -1.354, tRMSE: 0.6752, vLoss: 0.4574, vR2: -1.36, vRMSE: 0.6763\n",
      "[Epoch 90/500, t: 9.18s]:tLoss: 0.4546, tR2: -1.347, tRMSE: 0.6742, vLoss: 0.4561, vR2: -1.354, vRMSE: 0.6753\n",
      "[Epoch 91/500, t: 9.2s]:tLoss: 0.4533, tR2: -1.34, tRMSE: 0.6733, vLoss: 0.4548, vR2: -1.347, vRMSE: 0.6744\n",
      "[Epoch 92/500, t: 9.53s]:tLoss: 0.4520, tR2: -1.334, tRMSE: 0.6723, vLoss: 0.4535, vR2: -1.341, vRMSE: 0.6734\n",
      "[Epoch 93/500, t: 9.3s]:tLoss: 0.4507, tR2: -1.327, tRMSE: 0.6713, vLoss: 0.4522, vR2: -1.334, vRMSE: 0.6724\n",
      "[Epoch 94/500, t: 9.35s]:tLoss: 0.4494, tR2: -1.321, tRMSE: 0.6704, vLoss: 0.4509, vR2: -1.328, vRMSE: 0.6715\n",
      "[Epoch 95/500, t: 9.25s]:tLoss: 0.4481, tR2: -1.315, tRMSE: 0.6694, vLoss: 0.4496, vR2: -1.322, vRMSE: 0.6705\n",
      "[Epoch 96/500, t: 9.15s]:tLoss: 0.4469, tR2: -1.308, tRMSE: 0.6685, vLoss: 0.4484, vR2: -1.315, vRMSE: 0.6696\n",
      "[Epoch 97/500, t: 9.18s]:tLoss: 0.4456, tR2: -1.302, tRMSE: 0.6675, vLoss: 0.4471, vR2: -1.309, vRMSE: 0.6686\n",
      "[Epoch 98/500, t: 9.17s]:tLoss: 0.4444, tR2: -1.296, tRMSE: 0.6666, vLoss: 0.4458, vR2: -1.303, vRMSE: 0.6677\n",
      "[Epoch 99/500, t: 9.36s]:tLoss: 0.4431, tR2: -1.289, tRMSE: 0.6657, vLoss: 0.4446, vR2: -1.296, vRMSE: 0.6668\n",
      "[Epoch 100/500, t: 9.59s]:tLoss: 0.4419, tR2: -1.283, tRMSE: 0.6647, vLoss: 0.4433, vR2: -1.29, vRMSE: 0.6658\n",
      "[Epoch 101/500, t: 9.35s]:tLoss: 0.4406, tR2: -1.277, tRMSE: 0.6638, vLoss: 0.4421, vR2: -1.284, vRMSE: 0.6649\n",
      "[Epoch 102/500, t: 9.24s]:tLoss: 0.4394, tR2: -1.271, tRMSE: 0.6629, vLoss: 0.4409, vR2: -1.278, vRMSE: 0.6640\n",
      "[Epoch 103/500, t: 9.17s]:tLoss: 0.4382, tR2: -1.265, tRMSE: 0.6619, vLoss: 0.4396, vR2: -1.272, vRMSE: 0.6630\n",
      "[Epoch 104/500, t: 9.25s]:tLoss: 0.4369, tR2: -1.259, tRMSE: 0.6610, vLoss: 0.4384, vR2: -1.266, vRMSE: 0.6621\n",
      "[Epoch 105/500, t: 9.31s]:tLoss: 0.4357, tR2: -1.252, tRMSE: 0.6601, vLoss: 0.4372, vR2: -1.259, vRMSE: 0.6612\n",
      "[Epoch 106/500, t: 9.2s]:tLoss: 0.4345, tR2: -1.246, tRMSE: 0.6592, vLoss: 0.4360, vR2: -1.253, vRMSE: 0.6603\n",
      "[Epoch 107/500, t: 9.23s]:tLoss: 0.4333, tR2: -1.24, tRMSE: 0.6582, vLoss: 0.4347, vR2: -1.247, vRMSE: 0.6594\n",
      "[Epoch 108/500, t: 9.42s]:tLoss: 0.4321, tR2: -1.234, tRMSE: 0.6573, vLoss: 0.4335, vR2: -1.241, vRMSE: 0.6584\n",
      "[Epoch 109/500, t: 9.56s]:tLoss: 0.4309, tR2: -1.228, tRMSE: 0.6564, vLoss: 0.4323, vR2: -1.235, vRMSE: 0.6575\n",
      "[Epoch 110/500, t: 9.18s]:tLoss: 0.4297, tR2: -1.222, tRMSE: 0.6555, vLoss: 0.4311, vR2: -1.229, vRMSE: 0.6566\n",
      "[Epoch 111/500, t: 9.23s]:tLoss: 0.4285, tR2: -1.216, tRMSE: 0.6546, vLoss: 0.4299, vR2: -1.223, vRMSE: 0.6557\n",
      "[Epoch 112/500, t: 9.24s]:tLoss: 0.4273, tR2: -1.21, tRMSE: 0.6537, vLoss: 0.4288, vR2: -1.217, vRMSE: 0.6548\n",
      "[Epoch 113/500, t: 9.39s]:tLoss: 0.4261, tR2: -1.204, tRMSE: 0.6528, vLoss: 0.4276, vR2: -1.212, vRMSE: 0.6539\n",
      "[Epoch 114/500, t: 9.26s]:tLoss: 0.4249, tR2: -1.198, tRMSE: 0.6519, vLoss: 0.4264, vR2: -1.206, vRMSE: 0.6530\n",
      "[Epoch 115/500, t: 9.15s]:tLoss: 0.4238, tR2: -1.193, tRMSE: 0.6510, vLoss: 0.4252, vR2: -1.2, vRMSE: 0.6521\n",
      "[Epoch 116/500, t: 9.18s]:tLoss: 0.4226, tR2: -1.187, tRMSE: 0.6501, vLoss: 0.4240, vR2: -1.194, vRMSE: 0.6512\n",
      "[Epoch 117/500, t: 9.35s]:tLoss: 0.4214, tR2: -1.181, tRMSE: 0.6492, vLoss: 0.4229, vR2: -1.188, vRMSE: 0.6503\n",
      "[Epoch 118/500, t: 9.66s]:tLoss: 0.4203, tR2: -1.175, tRMSE: 0.6483, vLoss: 0.4217, vR2: -1.182, vRMSE: 0.6494\n",
      "[Epoch 119/500, t: 9.38s]:tLoss: 0.4191, tR2: -1.169, tRMSE: 0.6474, vLoss: 0.4205, vR2: -1.176, vRMSE: 0.6485\n",
      "[Epoch 120/500, t: 9.4s]:tLoss: 0.4180, tR2: -1.163, tRMSE: 0.6465, vLoss: 0.4194, vR2: -1.171, vRMSE: 0.6476\n",
      "[Epoch 121/500, t: 9.37s]:tLoss: 0.4168, tR2: -1.158, tRMSE: 0.6456, vLoss: 0.4182, vR2: -1.165, vRMSE: 0.6467\n",
      "[Epoch 122/500, t: 9.36s]:tLoss: 0.4157, tR2: -1.152, tRMSE: 0.6447, vLoss: 0.4171, vR2: -1.159, vRMSE: 0.6458\n",
      "[Epoch 123/500, t: 9.44s]:tLoss: 0.4145, tR2: -1.146, tRMSE: 0.6438, vLoss: 0.4159, vR2: -1.153, vRMSE: 0.6449\n",
      "[Epoch 124/500, t: 9.4s]:tLoss: 0.4134, tR2: -1.141, tRMSE: 0.6429, vLoss: 0.4148, vR2: -1.148, vRMSE: 0.6440\n",
      "[Epoch 125/500, t: 9.39s]:tLoss: 0.4122, tR2: -1.135, tRMSE: 0.6421, vLoss: 0.4137, vR2: -1.142, vRMSE: 0.6432\n",
      "[Epoch 126/500, t: 9.43s]:tLoss: 0.4111, tR2: -1.129, tRMSE: 0.6412, vLoss: 0.4125, vR2: -1.136, vRMSE: 0.6423\n",
      "[Epoch 127/500, t: 9.3s]:tLoss: 0.4100, tR2: -1.124, tRMSE: 0.6403, vLoss: 0.4114, vR2: -1.131, vRMSE: 0.6414\n",
      "[Epoch 128/500, t: 9.35s]:tLoss: 0.4089, tR2: -1.118, tRMSE: 0.6394, vLoss: 0.4103, vR2: -1.125, vRMSE: 0.6405\n",
      "[Epoch 129/500, t: 9.24s]:tLoss: 0.4078, tR2: -1.112, tRMSE: 0.6386, vLoss: 0.4092, vR2: -1.12, vRMSE: 0.6397\n",
      "[Epoch 130/500, t: 9.18s]:tLoss: 0.4067, tR2: -1.107, tRMSE: 0.6377, vLoss: 0.4081, vR2: -1.114, vRMSE: 0.6388\n",
      "[Epoch 131/500, t: 9.16s]:tLoss: 0.4055, tR2: -1.101, tRMSE: 0.6368, vLoss: 0.4070, vR2: -1.109, vRMSE: 0.6379\n",
      "[Epoch 132/500, t: 9.41s]:tLoss: 0.4045, tR2: -1.096, tRMSE: 0.6360, vLoss: 0.4059, vR2: -1.103, vRMSE: 0.6371\n",
      "[Epoch 133/500, t: 9.42s]:tLoss: 0.4034, tR2: -1.09, tRMSE: 0.6351, vLoss: 0.4048, vR2: -1.098, vRMSE: 0.6362\n",
      "[Epoch 134/500, t: 9.65s]:tLoss: 0.4023, tR2: -1.085, tRMSE: 0.6342, vLoss: 0.4037, vR2: -1.092, vRMSE: 0.6354\n",
      "[Epoch 135/500, t: 9.83s]:tLoss: 0.4012, tR2: -1.08, tRMSE: 0.6334, vLoss: 0.4026, vR2: -1.087, vRMSE: 0.6345\n",
      "[Epoch 136/500, t: 9.53s]:tLoss: 0.4001, tR2: -1.074, tRMSE: 0.6325, vLoss: 0.4015, vR2: -1.081, vRMSE: 0.6337\n",
      "[Epoch 137/500, t: 9.57s]:tLoss: 0.3990, tR2: -1.069, tRMSE: 0.6317, vLoss: 0.4005, vR2: -1.076, vRMSE: 0.6328\n",
      "[Epoch 138/500, t: 9.51s]:tLoss: 0.3980, tR2: -1.064, tRMSE: 0.6309, vLoss: 0.3994, vR2: -1.071, vRMSE: 0.6320\n",
      "[Epoch 139/500, t: 9.59s]:tLoss: 0.3969, tR2: -1.058, tRMSE: 0.6300, vLoss: 0.3983, vR2: -1.066, vRMSE: 0.6311\n",
      "[Epoch 140/500, t: 9.56s]:tLoss: 0.3959, tR2: -1.053, tRMSE: 0.6292, vLoss: 0.3973, vR2: -1.06, vRMSE: 0.6303\n",
      "[Epoch 141/500, t: 9.56s]:tLoss: 0.3948, tR2: -1.048, tRMSE: 0.6284, vLoss: 0.3963, vR2: -1.055, vRMSE: 0.6295\n",
      "[Epoch 142/500, t: 9.56s]:tLoss: 0.3938, tR2: -1.043, tRMSE: 0.6275, vLoss: 0.3952, vR2: -1.05, vRMSE: 0.6287\n",
      "[Epoch 143/500, t: 9.6s]:tLoss: 0.3928, tR2: -1.037, tRMSE: 0.6267, vLoss: 0.3942, vR2: -1.045, vRMSE: 0.6278\n",
      "[Epoch 144/500, t: 9.32s]:tLoss: 0.3917, tR2: -1.032, tRMSE: 0.6259, vLoss: 0.3932, vR2: -1.04, vRMSE: 0.6270\n",
      "[Epoch 145/500, t: 9.61s]:tLoss: 0.3907, tR2: -1.027, tRMSE: 0.6251, vLoss: 0.3921, vR2: -1.035, vRMSE: 0.6262\n",
      "[Epoch 146/500, t: 9.52s]:tLoss: 0.3897, tR2: -1.022, tRMSE: 0.6243, vLoss: 0.3911, vR2: -1.029, vRMSE: 0.6254\n",
      "[Epoch 147/500, t: 9.42s]:tLoss: 0.3887, tR2: -1.017, tRMSE: 0.6234, vLoss: 0.3901, vR2: -1.024, vRMSE: 0.6246\n",
      "[Epoch 148/500, t: 9.37s]:tLoss: 0.3877, tR2: -1.012, tRMSE: 0.6226, vLoss: 0.3891, vR2: -1.019, vRMSE: 0.6238\n",
      "[Epoch 149/500, t: 9.33s]:tLoss: 0.3867, tR2: -1.007, tRMSE: 0.6218, vLoss: 0.3881, vR2: -1.014, vRMSE: 0.6230\n",
      "[Epoch 150/500, t: 9.36s]:tLoss: 0.3857, tR2: -1.002, tRMSE: 0.6210, vLoss: 0.3871, vR2: -1.009, vRMSE: 0.6222\n",
      "[Epoch 151/500, t: 9.44s]:tLoss: 0.3847, tR2: -0.997, tRMSE: 0.6202, vLoss: 0.3861, vR2: -1.004, vRMSE: 0.6214\n",
      "[Epoch 152/500, t: 9.7s]:tLoss: 0.3837, tR2: -0.992, tRMSE: 0.6194, vLoss: 0.3851, vR2: -0.999, vRMSE: 0.6206\n",
      "[Epoch 153/500, t: 9.41s]:tLoss: 0.3827, tR2: -0.987, tRMSE: 0.6186, vLoss: 0.3841, vR2: -0.994, vRMSE: 0.6198\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 27\u001B[0m\n\u001B[1;32m     25\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, result \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(tr_outputs):\n\u001B[0;32m---> 27\u001B[0m     result_ \u001B[38;5;241m=\u001B[39m \u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39mtolist()[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     28\u001B[0m     tr_y_pred\u001B[38;5;241m.\u001B[39mappend(result_)\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, label \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(tr_labels):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
