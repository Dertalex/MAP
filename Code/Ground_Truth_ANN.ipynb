{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-27T12:19:53.964515Z",
     "start_time": "2025-02-27T12:19:53.962640Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from sympy.core.random import shuffle\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import math\n",
    "import src.generate_encodings as ge\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "id": "a190fdb085744e25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T12:20:02.185978Z",
     "start_time": "2025-02-27T12:20:02.183412Z"
    }
   },
   "source": [
    "print(torch.cuda.is_available())\n",
    "torch.cuda.get_device_name(0)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4060 Ti'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T12:20:03.842912Z",
     "start_time": "2025-02-27T12:20:03.478111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_set = \"../Data/Protein_Gym_Datasets/HIS7_YEAST_Pokusaeva_2019.csv\"\n",
    "\n",
    "to_encode = []\n",
    "with open(data_set, \"r\") as data_file:\n",
    "    lines = data_file.readlines()[1:]\n",
    "for line in lines:\n",
    "    to_encode.append(line.split(\",\")[1])"
   ],
   "id": "f2ee142b49949f3f",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T12:20:07.088276Z",
     "start_time": "2025-02-27T12:20:06.629806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_pairs = []\n",
    "embeddings = ge.load_encodings(\"../Data/Embeddings/HIS7_YEAST_Pokusaeva_2019/esm1b\")\n",
    "\n",
    "print(\"embeddings loaded\")\n",
    "for i, line in enumerate(lines):\n",
    "#     embedding = ge.generate_sequence_encodings(\"georgiev\", [line.split(\",\")[1]])[0]\n",
    "    score = line.split(\",\")[2]\n",
    "    embedding = embeddings[i]\n",
    "    data_pairs.append((embedding, float(score)))\n",
    "# del embeddings\n",
    "print(data_pairs[0])\n",
    "print(data_pairs[1])"
   ],
   "id": "6e66ee356053b7bc",
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '../Data/Embeddings/HIS7_YEAST_Pokusaeva_2019/esm1b/batch_001'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIsADirectoryError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[70], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m data_pairs \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m----> 2\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mge\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_encodings\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m../Data/Embeddings/HIS7_YEAST_Pokusaeva_2019/esm1b\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124membeddings loaded\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, line \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(lines):\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m#     embedding = ge.generate_sequence_encodings(\"georgiev\", [line.split(\",\")[1]])[0]\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Enzyme_Activity_Prediction/02_Playground/MAP/Code/src/generate_encodings.py:231\u001B[0m, in \u001B[0;36mload_encodings\u001B[0;34m(encodings_folder)\u001B[0m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m    230\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m saved_encoding \u001B[38;5;129;01min\u001B[39;00m saved_encodings:\n\u001B[0;32m--> 231\u001B[0m         encodings\u001B[38;5;241m.\u001B[39mappend(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mencodings_folder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msaved_encoding\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    233\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m encodings\n",
      "File \u001B[0;32m/opt/conda/conda/envs/MAP/lib/python3.12/site-packages/torch/serialization.py:1425\u001B[0m, in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[1;32m   1422\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m   1423\u001B[0m     pickle_load_args[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1425\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[1;32m   1426\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[1;32m   1427\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[1;32m   1428\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[1;32m   1429\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[1;32m   1430\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[0;32m/opt/conda/conda/envs/MAP/lib/python3.12/site-packages/torch/serialization.py:751\u001B[0m, in \u001B[0;36m_open_file_like\u001B[0;34m(name_or_buffer, mode)\u001B[0m\n\u001B[1;32m    749\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[1;32m    750\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[0;32m--> 751\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    752\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    753\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[0;32m/opt/conda/conda/envs/MAP/lib/python3.12/site-packages/torch/serialization.py:732\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[0;34m(self, name, mode)\u001B[0m\n\u001B[1;32m    731\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[0;32m--> 732\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mIsADirectoryError\u001B[0m: [Errno 21] Is a directory: '../Data/Embeddings/HIS7_YEAST_Pokusaeva_2019/esm1b/batch_001'"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T15:24:42.996357Z",
     "start_time": "2025-02-26T15:24:42.553512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "ef439f2fe9420b5d",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T15:24:43.130092Z",
     "start_time": "2025-02-26T15:24:42.998251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "random.shuffle(data_pairs)\n",
    "\n",
    "split_ratio = (0.75, 0.15, 0.10)\n",
    "split_train = int(len(data_pairs) * split_ratio[0])\n",
    "split_valid = int(len(data_pairs) * (split_ratio[1] + split_ratio[0]))\n",
    "\n",
    "x_train = torch.tensor([xy[0].cpu().reshape(-1) for xy in data_pairs[0:split_train]], dtype=torch.float32)\n",
    "x_valid = torch.tensor([xy[0].cpu().reshape(-1) for xy in data_pairs[split_train:split_valid]],\n",
    "                       dtype=torch.float32)\n",
    "x_test = torch.tensor([xy[0].cpu().reshape(-1) for xy in data_pairs[split_valid:]], dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor([[float(xy[1])] for xy in data_pairs[0:split_train]], dtype=torch.float32)\n",
    "y_valid = torch.tensor([[float(xy[1])] for xy in data_pairs[split_train: split_valid]], dtype=torch.float32)\n",
    "y_test = torch.tensor([[float(xy[1])] for xy in data_pairs[split_valid:]], dtype=torch.float32)\n",
    "\n",
    "# x_train = torch.tensor(np.array([xy[0].cpu().reshape(-1) for xy in data_pairs[0:split_train]]), dtype=torch.float32)\n",
    "# x_valid = torch.tensor(np.array([xy[0].cpu().reshape(-1) for xy in data_pairs[split_train:split_valid]]),\n",
    "#                        dtype=torch.float32)\n",
    "# x_test = torch.tensor(np.array([xy[0].cpu().reshape(-1) for xy in data_pairs[split_valid:]]), dtype=torch.float32)\n",
    "#\n",
    "# y_train = torch.tensor(np.array([[float(xy[1])] for xy in data_pairs[0:split_train]]), dtype=torch.float32)\n",
    "# y_valid = torch.tensor(np.array([[float(xy[1])] for xy in data_pairs[split_train: split_valid]]), dtype=torch.float32)\n",
    "# y_test = torch.tensor(np.array([[float(xy[1])] for xy in data_pairs[split_valid:]]), dtype=torch.float32)\n",
    "\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=1000, shuffle=False)\n",
    "\n",
    "test_data = TensorDataset(x_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "valid_data = TensorDataset(x_valid, y_valid)\n",
    "valid_loader = DataLoader(valid_data, batch_size=32, shuffle=False)\n",
    "\n",
    "input_len = len(x_train[0])"
   ],
   "id": "616fb2e3c9943e08",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[65], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m split_train \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(data_pairs) \u001B[38;5;241m*\u001B[39m split_ratio[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m      5\u001B[0m split_valid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(data_pairs) \u001B[38;5;241m*\u001B[39m (split_ratio[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m split_ratio[\u001B[38;5;241m0\u001B[39m]))\n\u001B[0;32m----> 7\u001B[0m x_train \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([\u001B[43mxy\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m()\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m xy \u001B[38;5;129;01min\u001B[39;00m data_pairs[\u001B[38;5;241m0\u001B[39m:split_train]], dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m      8\u001B[0m x_valid \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([xy[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m xy \u001B[38;5;129;01min\u001B[39;00m data_pairs[split_train:split_valid]],\n\u001B[1;32m      9\u001B[0m                        dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m     10\u001B[0m x_test \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([xy[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m xy \u001B[38;5;129;01min\u001B[39;00m data_pairs[split_valid:]], dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'numpy.ndarray' object has no attribute 'cpu'"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "id": "7a5a3e14619e2b3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T15:24:43.131379154Z",
     "start_time": "2025-02-26T15:18:29.119660Z"
    }
   },
   "source": [
    "class FFNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FFNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_len, input_len - 1)\n",
    "        self.fc2 = nn.Linear(input_len - 1, input_len)\n",
    "        self.fc3 = nn.Linear(input_len, input_len - 1)\n",
    "        self.fc4 = nn.Linear(input_len - 1, input_len)\n",
    "        self.fc5 = nn.Linear(input_len, input_len - 1)\n",
    "        self.fc6 = nn.Linear(input_len - 1, input_len)\n",
    "        self.fc7 = nn.Linear(input_len, input_len - 1)\n",
    "        self.fc8 = nn.Linear(input_len - 1, input_len)\n",
    "        self.fc9 = nn.Linear(input_len, input_len - 1)\n",
    "        self.fc10 = nn.Linear(input_len - 1, input_len)\n",
    "        self.fc11 = nn.Linear(input_len, input_len - 1)\n",
    "        self.output_layer = nn.Linear(input_len - 1, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = torch.relu(self.fc6(x))\n",
    "        x = torch.relu(self.fc7(x))\n",
    "        x = torch.relu(self.fc8(x))\n",
    "        x = torch.relu(self.fc9(x))\n",
    "        x = torch.relu(self.fc10(x))\n",
    "        # x = torch.relu(self.fc11(x))\n",
    "\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T15:24:43.131601408Z",
     "start_time": "2025-02-26T15:18:31.052907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gt_model = FFNet()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "optim = \"SGD\"\n",
    "if optim == \"SGD\":\n",
    "    optimizer = torch.optim.SGD(gt_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "else:\n",
    "    optimizer = torch.optim.Adam(gt_model.parameters(), lr=learning_rate / 100)\n",
    "# scheduler = lr_scheduler.LRScheduler(optimizer)"
   ],
   "id": "749567a2b44c13b9",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T15:24:43.132727156Z",
     "start_time": "2025-02-26T15:18:32.196589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"Track Training Metrics\"\"\"\n",
    "time_stamp = datetime.datetime.now().strftime(\"%Y_%m_%d-%H_%M\")\n",
    "outfile = f\"../Models/Logs/{time_stamp}.txt\"\n",
    "with open(outfile, \"w\") as f:\n",
    "    f.write(f\"Optimizer: {optim}, learning_rate: {learning_rate}, momentum: {momentum} \\n\")\n",
    "    f.write(f\"Model: \\n {gt_model} \\n\")\n",
    "    f.write(\"\\n\")"
   ],
   "id": "37620690b4f74576",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T15:24:43.132845868Z",
     "start_time": "2025-02-26T15:18:33.146953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_epochs = 500\n",
    "diplay_every_n_epochs = 1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    gt_model.train()\n",
    "    running_loss = 0.0\n",
    "    tr_y_pred = []\n",
    "    tr_y_true = []\n",
    "\n",
    "    best_val = 1\n",
    "    best_model = None\n",
    "\n",
    "    for tr_inputs, tr_labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        tr_outputs = gt_model.forward(tr_inputs)\n",
    "        loss = criterion(tr_outputs, tr_labels)\n",
    "        running_loss += loss.item()\n",
    "        tr_y_pred.append(tr_outputs)\n",
    "        tr_y_true.append(tr_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # scheduler.step()\n",
    "\n",
    "    gt_model.eval()\n",
    "    with torch.no_grad():\n",
    "        te_loss = 0.0\n",
    "        running_va_loss = 0.0\n",
    "\n",
    "        va_y_pred = []\n",
    "        va_y_true = []\n",
    "\n",
    "        for va_input, va_label in valid_loader:\n",
    "            va_outputs = gt_model.forward(va_input)\n",
    "            va_loss = criterion(va_outputs, va_label)\n",
    "            running_va_loss += va_loss.item()\n",
    "            va_y_pred.append(va_outputs)\n",
    "            va_y_true.append(va_label)\n",
    "\n",
    "        tr_y_true = torch.cat(tr_y_true).detach().numpy()\n",
    "        tr_y_pred = torch.cat(tr_y_pred).detach().numpy()\n",
    "        tr_RMSE = math.sqrt(mean_squared_error(tr_y_true, tr_y_pred))\n",
    "\n",
    "        va_y_true = torch.cat(va_y_true).detach().numpy()\n",
    "        va_y_pred = torch.cat(va_y_pred).detach().numpy()\n",
    "        va_RMSE = math.sqrt(mean_squared_error(va_y_true, va_y_pred))\n",
    "\n",
    "    with open(outfile, \"a\") as f:\n",
    "        f.write(\n",
    "            f'[Epoch {epoch + 1}/{n_epochs}, t: {round(time.time() - start_time, 2)}s]:Train Loss: {running_loss / len(train_loader):.4f}, '\n",
    "            f'Train RMSE: {tr_RMSE:.4f}, Valid Loss: {running_va_loss / len(valid_loader):.4f}, Valid RMSE: {va_RMSE:.4f} \\n')\n",
    "\n",
    "    if (epoch + 1) % diplay_every_n_epochs == 0:\n",
    "        print(\n",
    "            f'[Epoch {epoch + 1}/{n_epochs}, t: {round(time.time() - start_time, 2)}s]:Train Loss: {running_loss / len(train_loader):.4f}, '\n",
    "            f'Train RMSE: {tr_RMSE:.4f}, Valid Loss: {running_va_loss / len(valid_loader):.4f}, Valid RMSE: {va_RMSE:.4f} ')"
   ],
   "id": "7f7b0f2343212e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/500, t: 49.37s]:Train Loss: 0.2254, Train RMSE: 0.4749, Valid Loss: 0.2021, Valid RMSE: 0.4495 \n",
      "[Epoch 2/500, t: 50.45s]:Train Loss: 0.2015, Train RMSE: 0.4489, Valid Loss: 0.2021, Valid RMSE: 0.4495 \n",
      "[Epoch 3/500, t: 50.45s]:Train Loss: 0.2015, Train RMSE: 0.4489, Valid Loss: 0.2021, Valid RMSE: 0.4495 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[61], line 21\u001B[0m\n\u001B[1;32m     19\u001B[0m     tr_y_pred\u001B[38;5;241m.\u001B[39mappend(tr_outputs)\n\u001B[1;32m     20\u001B[0m     tr_y_true\u001B[38;5;241m.\u001B[39mappend(tr_labels)\n\u001B[0;32m---> 21\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# scheduler.step()\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/conda/envs/MAP/lib/python3.12/site-packages/torch/_tensor.py:626\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    616\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    617\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    618\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    619\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    624\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    625\u001B[0m     )\n\u001B[0;32m--> 626\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    627\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    628\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/conda/envs/MAP/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/conda/envs/MAP/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    821\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 823\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    824\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    825\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    826\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    827\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 61
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
